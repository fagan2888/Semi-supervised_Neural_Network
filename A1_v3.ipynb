{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pickle \n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "from torch.legacy.nn import ParallelCriterion\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "trainset_labeled = pickle.load(open(\"train_labeled.p\", \"rb\"))\n",
    "validset = pickle.load(open(\"validation.p\", \"rb\"))\n",
    "trainset_unlabeled = pickle.load(open(\"train_unlabeled.p\", \"rb\"))\n",
    "train_loader = torch.utils.data.DataLoader(trainset_labeled, batch_size=64, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(validset, batch_size=64, shuffle=True)\n",
    "unlabeled_loader = torch.utils.data.DataLoader(trainset_unlabeled, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Denoise(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Denoise, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(1, 10))\n",
    "        self.reset_parameter()\n",
    "        \n",
    "    def forward(self, input_n, input_u):\n",
    "\n",
    "        dims = input_n.size()\n",
    "        a1 = self.expand_var(dims, self.weight[0,0])\n",
    "        a2 = self.expand_var(dims, self.weight[0,1])\n",
    "        a3 = self.expand_var(dims, self.weight[0,2])\n",
    "        a4 = self.expand_var(dims, self.weight[0,3])\n",
    "        a5 = self.expand_var(dims, self.weight[0,4])\n",
    "        a6 = self.expand_var(dims, self.weight[0,5])\n",
    "        a7 = self.expand_var(dims, self.weight[0,6])\n",
    "        a8 = self.expand_var(dims, self.weight[0,7])\n",
    "        a9 = self.expand_var(dims, self.weight[0,8])\n",
    "        a10 = self.expand_var(dims, self.weight[0,9])\n",
    "        \n",
    "        mu = Variable(torch.zeros(input_u.size()))\n",
    "        nu = Variable(torch.zeros(input_u.size()))\n",
    "        mu = a1 * torch.sigmoid(a2 * input_u + a3) + a4 * input_u + a5\n",
    "        nu = a6 * torch.sigmoid(a7 * input_u + a8) + a9 * input_u + a10\n",
    "        output = (input_n - mu) * nu + mu\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def reset_parameter(self):\n",
    "        stdv = 0.1\n",
    "        self.weight.data.uniform_(-stdv, stdv)  \n",
    "        \n",
    "    def expand_var(self, dims, weight_i):\n",
    "        if len(dims) == 2:\n",
    "            output = weight_i.unsqueeze(1).expand(dims)\n",
    "        elif len(dims) == 4:\n",
    "            output = weight_i.unsqueeze(1).unsqueeze(2).unsqueeze(3).expand(dims)      \n",
    "        return output\n",
    "\n",
    "        \n",
    "class normalize(Function):\n",
    "        \n",
    "    def forward(self, z_pre, input_z):\n",
    "        m = torch.mean(z_pre, 0)\n",
    "        s = torch.std(z_pre, 0) + 1e-5\n",
    "            \n",
    "        dims = z_pre.size()\n",
    "        print(s.size())\n",
    "        print(z_pre.size())\n",
    "        z_m = s.expand(dims)\n",
    "        z_s = s.expand(dims)\n",
    "            \n",
    "        return (input_z - z_m) / z_s\n",
    "\n",
    "        \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Pre layer\n",
    "        self.bn0 = nn.BatchNorm2d(1, affine = False)\n",
    "        \n",
    "        # First layer\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.bn1 = nn.BatchNorm2d(10, affine = False)\n",
    "        self.bn1_n = nn.BatchNorm2d(10, affine = False)\n",
    "        # Second layer\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.bn2 = nn.BatchNorm2d(20, affine = False)\n",
    "        self.bn2_n = nn.BatchNorm2d(20, affine = False)\n",
    "        #self.conv2_drop = nn.Dropout2d()\n",
    "        # Third layer\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.bn3 = nn.BatchNorm1d(50, affine = False)\n",
    "        self.bn3_n = nn.BatchNorm1d(50, affine = False)\n",
    "        # Fourth layer\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        self.bn4 = nn.BatchNorm1d(10, affine = False)\n",
    "        self.bn4_n = nn.BatchNorm1d(10, affine = False)\n",
    "        \n",
    "        # Denoise Path\n",
    "        self.denoise4 = Denoise()\n",
    "        # Fourth layer\n",
    "        self.fc2_b = nn.Linear(10, 50)\n",
    "        self.bn4_b1 = nn.BatchNorm1d(10, affine = False)\n",
    "        self.bn4_b2 = nn.BatchNorm1d(10, affine = False)\n",
    "        #self.normalize4 = normalize()\n",
    "        # Third layer\n",
    "        self.denoise3 = Denoise()\n",
    "        self.fc1_b = nn.Linear(50, 320)\n",
    "        self.bn3_b1 = nn.BatchNorm1d(50, affine = False)\n",
    "        self.bn3_b2 = nn.BatchNorm1d(50, affine = False)\n",
    "        #self.normalize3 = normalize()\n",
    "        # Second layer\n",
    "        self.denoise2 = Denoise()\n",
    "        self.conv2t = nn.ConvTranspose2d(20, 10, kernel_size = 5)\n",
    "        self.bn2_b1 = nn.BatchNorm2d(20, affine = False)\n",
    "        self.bn2_b2 = nn.BatchNorm2d(20, affine = False)\n",
    "        #self.normalize2 = normalize()\n",
    "        # First layer\n",
    "        self.denoise1 = Denoise()\n",
    "        self.conv1t = nn.ConvTranspose2d(10, 1, kernel_size = 5)\n",
    "        self.bn1_b1 = nn.BatchNorm2d(10, affine = False)\n",
    "        self.bn1_b2 = nn.BatchNorm2d(10, affine = False)\n",
    "        #self.normalize1 = normalize()\n",
    "        self.denoise0 = Denoise()\n",
    "        self.bn0_b1 = nn.BatchNorm2d(1, affine = False)\n",
    "        self.bn0_b2 = nn.BatchNorm2d(1, affine = False)\n",
    "        #self.normalize0 = normalize()\n",
    "        # Loss Function for Unsupervised Learning\n",
    "        #self.batch_ave_norm = BN()\n",
    "    \n",
    "    def forward(self, x, labeled = True):\n",
    "        \n",
    "        def batch_ave_norm(z_c, z_h, batch_size):\n",
    "        # compute the l-2 norm of (z_hat - z)\n",
    "            dims = z_c.size()\n",
    "            temp = Variable(torch.zeros(1))\n",
    "            for i in range(0, batch_size):\n",
    "                if len(dims) == 2:\n",
    "                    temp += torch.dist(z_c[i,:], z_h[i,:])   \n",
    "                    layer_width = dims[-1]\n",
    "                elif len(dims) == 3:\n",
    "                    temp += torch.dist(z_c[i,:,:], z_h[i,:,:]) \n",
    "                    layer_width = torch.cumprod(torch.Tensor([dims[1], dims[2]]),0)[-1]\n",
    "                elif len(dims) == 4:\n",
    "                    temp += torch.dist(z_c[i,:,:,:], z_h[i,:,:,:])\n",
    "                    layer_width = torch.cumprod(torch.Tensor([dims[1], dims[2], dims[3]]),0)[-1]  \n",
    "            output = temp/(batch_size*layer_width)\n",
    "            return output\n",
    "        \n",
    "        def normalize(z_pre, input_z):\n",
    "            z = z_pre\n",
    "            \n",
    "            m = torch.mean(z_pre, 0)\n",
    "            z_s = z_pre\n",
    "            temp = torch.std(z_pre.data, 0) + 1e-5\n",
    "            \n",
    "            dims = z_pre.size()\n",
    "            z_m = m.expand(dims)\n",
    "            z_s.data = temp.expand(dims)\n",
    "            return (input_z - z_m) / z_s\n",
    "        \n",
    "        def noise(x):\n",
    "            n = torch.randn(x.size()) * 0.1\n",
    "            return n\n",
    "        \n",
    "        batch_size = Variable(torch.Tensor([x.size()[0]]))\n",
    "        batch_size = x.size()[0]\n",
    "        ###### clean path ######\n",
    "        ### Level 0\n",
    "        h_c0 = self.bn0(x)\n",
    "        ### Level 1\n",
    "        x_c1 = F.max_pool2d(self.conv1(x), 2)\n",
    "        z_c1 = self.bn1(x_c1)\n",
    "        h_c1 = F.relu(z_c1)\n",
    "        ### Level 2\n",
    "        x_c2 = F.max_pool2d(self.conv2(h_c1), 2)\n",
    "        z_c2 = self.bn2(x_c2)\n",
    "        h_c2 = F.relu(z_c2)\n",
    "        ### Level 3\n",
    "        x_c3 = h_c2.view(-1, 320)\n",
    "        x_c3 = self.fc1(x_c3)\n",
    "        z_c3 = self.bn3(x_c3)\n",
    "        h_c3 = F.relu(z_c3)\n",
    "        ### Level 4\n",
    "        x_c4 = self.fc2(h_c3)\n",
    "        z_c4 = self.bn4(x_c4)\n",
    "        h_c4 = F.relu(z_c4)\n",
    "        \n",
    "        \n",
    "        ###### noise path ######\n",
    "        ### Level 0\n",
    "        h_n0 = copy.deepcopy(x)\n",
    "        h_n0.data += noise(h_n0.data)\n",
    "        ### Level 1\n",
    "        x_n1, i_n1 = F.max_pool2d(self.conv1(h_n0), 2, return_indices = True)  # projection z_pre\n",
    "        z_n1 = self.bn1_n(x_n1)                                                  # normalize\n",
    "        z_n1.data += noise(x_n1.data)\n",
    "        h_n1 = F.relu(z_n1)\n",
    "        ### Level 2\n",
    "        x_n2, i_n2 = F.max_pool2d(self.conv2(h_n1), 2, return_indices = True)\n",
    "        z_n2 = self.bn2_n(x_n2)\n",
    "        z_n2.data += noise(x_n2.data)\n",
    "        h_n2 = F.relu(z_n2)\n",
    "        ### Level 3\n",
    "        x_n3 = h_n2.view(-1, 320)\n",
    "        x_n3 = self.fc1(x_n3) \n",
    "        z_n3 = self.bn3_n(x_n3) \n",
    "        z_n3.data += noise(x_n3.data)\n",
    "        h_n3 = F.relu(z_n3)\n",
    "        ### Level 4\n",
    "        x_n4 = self.fc2(h_n3)\n",
    "        z_n4 = self.bn4_n(x_n4)\n",
    "        z_n4.data += noise(x_n4.data)\n",
    "        h_n4 = F.relu(z_n4) \n",
    "        \n",
    "        ###### Decoder and denoising ######\n",
    "        ### Level 4\n",
    "        u_4 = h_n4\n",
    "        u_4 = self.bn4_b1(u_4)\n",
    "        z_d4 = self.denoise4(z_n4, u_4)\n",
    "        #z_dn4 = normalize(x_c4, z_d4)   # \"normalized\" for loss function\n",
    "        z_dn4 = self.bn4_b2(z_d4)\n",
    "        ### Level 3\n",
    "        #u_3 = self.bn3(self.fc2_b(u_4))\n",
    "        u_3 = self.bn3_b1(self.fc2_b(z_d4))\n",
    "        z_d3 = self.denoise3(z_n3, u_3)\n",
    "        #z_dn3 = normalize(x_c3, z_d3)\n",
    "        z_dn3 = self.bn3_b2(z_d3)\n",
    "        ### Level 2\n",
    "        #u_2 = self.fc1_b(u_3)\n",
    "        u_2 = self.fc1_b(z_d3)\n",
    "        dim_c2 = z_c2.size()\n",
    "        u_2 = self.bn2_b1(u_2.view(dim_c2))\n",
    "        z_d2 = self.denoise2(z_n2, u_2)\n",
    "        #z_dn2 = normalize(x_c2, z_d2)\n",
    "        z_dn2 = self.bn2_b2(z_d2)\n",
    "        ### Level 1\n",
    "        #u_1 = self.bn1(self.conv2t(F.max_unpool2d(u_2, i_n2, kernel_size = 2, stride = 2)))\n",
    "        u_1 = self.bn1_b1(self.conv2t(F.max_unpool2d(z_d2, i_n2, kernel_size = 2, stride = 2)))\n",
    "        z_d1 = self.denoise1(z_n1, u_1)\n",
    "        #z_dn1 = normalize(x_c1, z_d1)\n",
    "        z_dn1 = self.bn1_b2(z_d1)\n",
    "        ### Level 0\n",
    "        #u_0 = self.bn0(self.conv1t(F.max_unpool2d(u_1, i_n1, kernel_size = 2, stride = 2)))\n",
    "        u_0 = self.bn0_b1(self.conv1t(F.max_unpool2d(z_d1, i_n1, kernel_size = 2, stride = 2)))\n",
    "        z_d0 = self.denoise0(h_n0, u_0)\n",
    "        #z_dn0 = normalize(h_c0, z_d0)\n",
    "        z_dn0 = self.bn0_b2(z_d0)\n",
    "        \n",
    "        ### Unsupervised Loss\n",
    "        C0 = batch_ave_norm(h_c0, z_dn0, batch_size)\n",
    "        C1 = batch_ave_norm(z_c1, z_dn1, batch_size)\n",
    "        C2 = batch_ave_norm(z_c2, z_dn2, batch_size)\n",
    "        C3 = batch_ave_norm(z_c3, z_dn3, batch_size)\n",
    "        C4 = batch_ave_norm(z_c4, z_dn4, batch_size)\n",
    "        l0 = 10\n",
    "        l1 = 5\n",
    "        l2 = 5\n",
    "        l3 = 5\n",
    "        l4 = 10\n",
    "        C_u = C0*l0 + C1*l1 + C2*l2 + C3*l3 + C4*l4 \n",
    "        \n",
    "        if labeled == True:\n",
    "            C_d = F.log_softmax(h_n4)\n",
    "        else:\n",
    "            C_d = Variable(torch.zero(1))\n",
    "            \n",
    "        return C_d, C_u\n",
    "        \n",
    "model = Net()\n",
    "model.parameters\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        output, loss_u = model(data, labeled = True)\n",
    "        loss_s = F.nll_loss(output, target)\n",
    "        loss = loss_s + loss_u\n",
    "\n",
    "        loss.backward()\n",
    "            \n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "               \n",
    "    print(\"Train with unlabled data...\")            \n",
    "    for batch_idx, (data, target) in enumerate(unlabeled_loader):\n",
    "        if batch_idx < 100:\n",
    "            data = Variable(data)\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            output, loss = model(data, labeled = True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 10 == 0:\n",
    "                #print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                #    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                #    100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(unlabeled_loader.dataset),\n",
    "                    100. * batch_idx / len(unlabeled_loader), loss.data[0]))\n",
    "            \n",
    "def test(epoch, valid_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in valid_loader:\n",
    "\n",
    "        data, target = Variable(data, volatile = True), Variable(target)\n",
    "        \n",
    "        \n",
    "        output, loss_u = model(data) \n",
    "        test_loss = test_loss + F.nll_loss(output, target).data[0] + float(loss_u.data.numpy()[0])\n",
    "        \n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss /= len(valid_loader) # loss function already averages over batch size\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(valid_loader.dataset),\n",
    "        100. * correct / len(valid_loader.dataset)))       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/3000 (0%)]\tLoss: 1.056455\n",
      "Train Epoch: 1 [640/3000 (21%)]\tLoss: 1.049464\n",
      "Train Epoch: 1 [1280/3000 (43%)]\tLoss: 0.938352\n",
      "Train Epoch: 1 [1920/3000 (64%)]\tLoss: 0.963995\n",
      "Train Epoch: 1 [2560/3000 (85%)]\tLoss: 0.934807\n",
      "Train with unlabled data...\n",
      "Train Epoch: 1 [0/47000 (0%)]\tLoss: 0.457787\n",
      "Train Epoch: 1 [640/47000 (1%)]\tLoss: 0.407684\n",
      "Train Epoch: 1 [1280/47000 (3%)]\tLoss: 0.377911\n",
      "Train Epoch: 1 [1920/47000 (4%)]\tLoss: 0.416855\n",
      "Train Epoch: 1 [2560/47000 (5%)]\tLoss: 0.387680\n",
      "Train Epoch: 1 [3200/47000 (7%)]\tLoss: 0.391972\n",
      "Train Epoch: 1 [3840/47000 (8%)]\tLoss: 0.365002\n",
      "Train Epoch: 1 [4480/47000 (10%)]\tLoss: 0.392333\n",
      "Train Epoch: 1 [5120/47000 (11%)]\tLoss: 0.402331\n",
      "Train Epoch: 1 [5760/47000 (12%)]\tLoss: 0.390207\n",
      "\n",
      "Test set: Average loss: 0.8962, Accuracy: 9709/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [0/3000 (0%)]\tLoss: 1.012672\n",
      "Train Epoch: 2 [640/3000 (21%)]\tLoss: 1.022838\n",
      "Train Epoch: 2 [1280/3000 (43%)]\tLoss: 0.946368\n",
      "Train Epoch: 2 [1920/3000 (64%)]\tLoss: 0.927318\n",
      "Train Epoch: 2 [2560/3000 (85%)]\tLoss: 1.003062\n",
      "Train with unlabled data...\n",
      "Train Epoch: 2 [0/47000 (0%)]\tLoss: 0.375580\n",
      "Train Epoch: 2 [640/47000 (1%)]\tLoss: 0.440534\n",
      "Train Epoch: 2 [1280/47000 (3%)]\tLoss: 0.407128\n",
      "Train Epoch: 2 [1920/47000 (4%)]\tLoss: 0.405137\n",
      "Train Epoch: 2 [2560/47000 (5%)]\tLoss: 0.409571\n",
      "Train Epoch: 2 [3200/47000 (7%)]\tLoss: 0.370400\n",
      "Train Epoch: 2 [3840/47000 (8%)]\tLoss: 0.372138\n",
      "Train Epoch: 2 [4480/47000 (10%)]\tLoss: 0.399704\n",
      "Train Epoch: 2 [5120/47000 (11%)]\tLoss: 0.371238\n",
      "Train Epoch: 2 [5760/47000 (12%)]\tLoss: 0.376848\n",
      "\n",
      "Test set: Average loss: 0.9124, Accuracy: 9708/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [0/3000 (0%)]\tLoss: 0.871654\n",
      "Train Epoch: 3 [640/3000 (21%)]\tLoss: 0.941313\n",
      "Train Epoch: 3 [1280/3000 (43%)]\tLoss: 0.914359\n",
      "Train Epoch: 3 [1920/3000 (64%)]\tLoss: 0.984430\n",
      "Train Epoch: 3 [2560/3000 (85%)]\tLoss: 0.909865\n",
      "Train with unlabled data...\n",
      "Train Epoch: 3 [0/47000 (0%)]\tLoss: 0.439066\n",
      "Train Epoch: 3 [640/47000 (1%)]\tLoss: 0.382998\n",
      "Train Epoch: 3 [1280/47000 (3%)]\tLoss: 0.371765\n",
      "Train Epoch: 3 [1920/47000 (4%)]\tLoss: 0.416181\n",
      "Train Epoch: 3 [2560/47000 (5%)]\tLoss: 0.369990\n",
      "Train Epoch: 3 [3200/47000 (7%)]\tLoss: 0.357726\n",
      "Train Epoch: 3 [3840/47000 (8%)]\tLoss: 0.375995\n",
      "Train Epoch: 3 [4480/47000 (10%)]\tLoss: 0.416256\n",
      "Train Epoch: 3 [5120/47000 (11%)]\tLoss: 0.410148\n",
      "Train Epoch: 3 [5760/47000 (12%)]\tLoss: 0.387336\n",
      "\n",
      "Test set: Average loss: 0.9021, Accuracy: 9711/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/3000 (0%)]\tLoss: 0.867147\n",
      "Train Epoch: 4 [640/3000 (21%)]\tLoss: 0.969389\n",
      "Train Epoch: 4 [1280/3000 (43%)]\tLoss: 0.868007\n",
      "Train Epoch: 4 [1920/3000 (64%)]\tLoss: 0.937555\n",
      "Train Epoch: 4 [2560/3000 (85%)]\tLoss: 0.857780\n",
      "Train with unlabled data...\n",
      "Train Epoch: 4 [0/47000 (0%)]\tLoss: 0.406652\n",
      "Train Epoch: 4 [640/47000 (1%)]\tLoss: 0.392282\n",
      "Train Epoch: 4 [1280/47000 (3%)]\tLoss: 0.387715\n",
      "Train Epoch: 4 [1920/47000 (4%)]\tLoss: 0.370360\n",
      "Train Epoch: 4 [2560/47000 (5%)]\tLoss: 0.381897\n",
      "Train Epoch: 4 [3200/47000 (7%)]\tLoss: 0.386801\n",
      "Train Epoch: 4 [3840/47000 (8%)]\tLoss: 0.362525\n",
      "Train Epoch: 4 [4480/47000 (10%)]\tLoss: 0.380122\n",
      "Train Epoch: 4 [5120/47000 (11%)]\tLoss: 0.399362\n",
      "Train Epoch: 4 [5760/47000 (12%)]\tLoss: 0.439288\n",
      "\n",
      "Test set: Average loss: 0.8985, Accuracy: 9712/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/3000 (0%)]\tLoss: 0.908210\n",
      "Train Epoch: 5 [640/3000 (21%)]\tLoss: 1.026568\n",
      "Train Epoch: 5 [1280/3000 (43%)]\tLoss: 0.965948\n",
      "Train Epoch: 5 [1920/3000 (64%)]\tLoss: 0.845156\n",
      "Train Epoch: 5 [2560/3000 (85%)]\tLoss: 0.940906\n",
      "Train with unlabled data...\n",
      "Train Epoch: 5 [0/47000 (0%)]\tLoss: 0.380451\n",
      "Train Epoch: 5 [640/47000 (1%)]\tLoss: 0.379157\n",
      "Train Epoch: 5 [1280/47000 (3%)]\tLoss: 0.407950\n",
      "Train Epoch: 5 [1920/47000 (4%)]\tLoss: 0.373947\n",
      "Train Epoch: 5 [2560/47000 (5%)]\tLoss: 0.393678\n",
      "Train Epoch: 5 [3200/47000 (7%)]\tLoss: 0.411572\n",
      "Train Epoch: 5 [3840/47000 (8%)]\tLoss: 0.361723\n",
      "Train Epoch: 5 [4480/47000 (10%)]\tLoss: 0.392023\n",
      "Train Epoch: 5 [5120/47000 (11%)]\tLoss: 0.352380\n",
      "Train Epoch: 5 [5760/47000 (12%)]\tLoss: 0.405858\n",
      "\n",
      "Test set: Average loss: 0.9035, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/3000 (0%)]\tLoss: 0.970701\n",
      "Train Epoch: 6 [640/3000 (21%)]\tLoss: 0.896828\n",
      "Train Epoch: 6 [1280/3000 (43%)]\tLoss: 0.904938\n",
      "Train Epoch: 6 [1920/3000 (64%)]\tLoss: 0.957683\n",
      "Train Epoch: 6 [2560/3000 (85%)]\tLoss: 0.833405\n",
      "Train with unlabled data...\n",
      "Train Epoch: 6 [0/47000 (0%)]\tLoss: 0.386018\n",
      "Train Epoch: 6 [640/47000 (1%)]\tLoss: 0.392355\n",
      "Train Epoch: 6 [1280/47000 (3%)]\tLoss: 0.426738\n",
      "Train Epoch: 6 [1920/47000 (4%)]\tLoss: 0.408022\n",
      "Train Epoch: 6 [2560/47000 (5%)]\tLoss: 0.372917\n",
      "Train Epoch: 6 [3200/47000 (7%)]\tLoss: 0.425674\n",
      "Train Epoch: 6 [3840/47000 (8%)]\tLoss: 0.358266\n",
      "Train Epoch: 6 [4480/47000 (10%)]\tLoss: 0.393350\n",
      "Train Epoch: 6 [5120/47000 (11%)]\tLoss: 0.362727\n",
      "Train Epoch: 6 [5760/47000 (12%)]\tLoss: 0.363982\n",
      "\n",
      "Test set: Average loss: 0.9033, Accuracy: 9714/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [0/3000 (0%)]\tLoss: 0.934855\n",
      "Train Epoch: 7 [640/3000 (21%)]\tLoss: 1.132256\n",
      "Train Epoch: 7 [1280/3000 (43%)]\tLoss: 0.883575\n",
      "Train Epoch: 7 [1920/3000 (64%)]\tLoss: 1.041964\n",
      "Train Epoch: 7 [2560/3000 (85%)]\tLoss: 0.896276\n",
      "Train with unlabled data...\n",
      "Train Epoch: 7 [0/47000 (0%)]\tLoss: 0.412980\n",
      "Train Epoch: 7 [640/47000 (1%)]\tLoss: 0.433300\n",
      "Train Epoch: 7 [1280/47000 (3%)]\tLoss: 0.371164\n",
      "Train Epoch: 7 [1920/47000 (4%)]\tLoss: 0.410531\n",
      "Train Epoch: 7 [2560/47000 (5%)]\tLoss: 0.401097\n",
      "Train Epoch: 7 [3200/47000 (7%)]\tLoss: 0.379335\n",
      "Train Epoch: 7 [3840/47000 (8%)]\tLoss: 0.377391\n",
      "Train Epoch: 7 [4480/47000 (10%)]\tLoss: 0.369087\n",
      "Train Epoch: 7 [5120/47000 (11%)]\tLoss: 0.385667\n",
      "Train Epoch: 7 [5760/47000 (12%)]\tLoss: 0.362605\n",
      "\n",
      "Test set: Average loss: 0.9143, Accuracy: 9718/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [0/3000 (0%)]\tLoss: 0.886154\n",
      "Train Epoch: 8 [640/3000 (21%)]\tLoss: 0.977377\n",
      "Train Epoch: 8 [1280/3000 (43%)]\tLoss: 1.001342\n",
      "Train Epoch: 8 [1920/3000 (64%)]\tLoss: 0.937613\n",
      "Train Epoch: 8 [2560/3000 (85%)]\tLoss: 1.026133\n",
      "Train with unlabled data...\n",
      "Train Epoch: 8 [0/47000 (0%)]\tLoss: 0.382500\n",
      "Train Epoch: 8 [640/47000 (1%)]\tLoss: 0.363458\n",
      "Train Epoch: 8 [1280/47000 (3%)]\tLoss: 0.433463\n",
      "Train Epoch: 8 [1920/47000 (4%)]\tLoss: 0.387358\n",
      "Train Epoch: 8 [2560/47000 (5%)]\tLoss: 0.369353\n",
      "Train Epoch: 8 [3200/47000 (7%)]\tLoss: 0.363189\n",
      "Train Epoch: 8 [3840/47000 (8%)]\tLoss: 0.401982\n",
      "Train Epoch: 8 [4480/47000 (10%)]\tLoss: 0.383616\n",
      "Train Epoch: 8 [5120/47000 (11%)]\tLoss: 0.414396\n",
      "Train Epoch: 8 [5760/47000 (12%)]\tLoss: 0.377584\n",
      "\n",
      "Test set: Average loss: 0.9007, Accuracy: 9720/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [0/3000 (0%)]\tLoss: 0.884175\n",
      "Train Epoch: 9 [640/3000 (21%)]\tLoss: 1.058982\n",
      "Train Epoch: 9 [1280/3000 (43%)]\tLoss: 0.885928\n",
      "Train Epoch: 9 [1920/3000 (64%)]\tLoss: 0.906852\n",
      "Train Epoch: 9 [2560/3000 (85%)]\tLoss: 0.980224\n",
      "Train with unlabled data...\n",
      "Train Epoch: 9 [0/47000 (0%)]\tLoss: 0.382646\n",
      "Train Epoch: 9 [640/47000 (1%)]\tLoss: 0.380307\n",
      "Train Epoch: 9 [1280/47000 (3%)]\tLoss: 0.371186\n",
      "Train Epoch: 9 [1920/47000 (4%)]\tLoss: 0.394913\n",
      "Train Epoch: 9 [2560/47000 (5%)]\tLoss: 0.391893\n",
      "Train Epoch: 9 [3200/47000 (7%)]\tLoss: 0.416768\n",
      "Train Epoch: 9 [3840/47000 (8%)]\tLoss: 0.397963\n",
      "Train Epoch: 9 [4480/47000 (10%)]\tLoss: 0.381170\n",
      "Train Epoch: 9 [5120/47000 (11%)]\tLoss: 0.361551\n",
      "Train Epoch: 9 [5760/47000 (12%)]\tLoss: 0.382874\n",
      "\n",
      "Test set: Average loss: 0.9000, Accuracy: 9706/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [0/3000 (0%)]\tLoss: 0.989341\n",
      "Train Epoch: 10 [640/3000 (21%)]\tLoss: 0.964695\n",
      "Train Epoch: 10 [1280/3000 (43%)]\tLoss: 0.898855\n",
      "Train Epoch: 10 [1920/3000 (64%)]\tLoss: 0.927091\n",
      "Train Epoch: 10 [2560/3000 (85%)]\tLoss: 0.849351\n",
      "Train with unlabled data...\n",
      "Train Epoch: 10 [0/47000 (0%)]\tLoss: 0.483689\n",
      "Train Epoch: 10 [640/47000 (1%)]\tLoss: 0.387154\n",
      "Train Epoch: 10 [1280/47000 (3%)]\tLoss: 0.376294\n",
      "Train Epoch: 10 [1920/47000 (4%)]\tLoss: 0.381057\n",
      "Train Epoch: 10 [2560/47000 (5%)]\tLoss: 0.415408\n",
      "Train Epoch: 10 [3200/47000 (7%)]\tLoss: 0.408495\n",
      "Train Epoch: 10 [3840/47000 (8%)]\tLoss: 0.390745\n",
      "Train Epoch: 10 [4480/47000 (10%)]\tLoss: 0.370129\n",
      "Train Epoch: 10 [5120/47000 (11%)]\tLoss: 0.368088\n",
      "Train Epoch: 10 [5760/47000 (12%)]\tLoss: 0.392728\n",
      "\n",
      "Test set: Average loss: 0.8948, Accuracy: 9721/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1,11):\n",
    "    train(epoch)\n",
    "    test(epoch,valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
